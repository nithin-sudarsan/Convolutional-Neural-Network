{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    def forward(self, input):\n",
    "        # PURPOSE: return output\n",
    "        pass\n",
    "    def backward(self, output_gradient, learning_rate): # output_gradient = dE/dY ; learning_rate = alpha\n",
    "        # PURPOSE: update parameters and return input gradient (dE/dX)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.biases = np.random.randn(output_size, 1) \n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input \n",
    "        return np.dot(self.weights, self.input) + self.biases\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weight_gradient = np.dot(weight_gradient, self.input.T)\n",
    "        self.weights -= learning_rate * weight_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "        return np.dot(self.weights.T, output_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x : np.tanh(x)\n",
    "        tanh_prime = lambda x : 1 - (np.tanh(x) ** 2)\n",
    "        super().__init__(tanh, tanh_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth): \n",
    "        # Lets say input is a RGB image of dimension 24 pixels (height) * 24 pixels (width)\n",
    "        # So input_shape is 3 * 24 * 24\n",
    "        # For simplicity, let's consider kernel_size is 2 * 2\n",
    "        # Depth is the number of kernels we're using in each layer of the kernel\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        # Now output shape can be calculated using the formula Y = I - K + 1 on height and width of the input shape\n",
    "        # Here Y is the shape of output matrix, I is the shape of input matrix and K is the shape of kernel\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size +1)\n",
    "        # Lets say there are 2 layers of kernel and we know the depth of input image (input_depth) is 3, then depth is 2\n",
    "        # Then the overall kernel shape is dept(2) * input_depth (3) * kernel_size(2*2) * kernel_size(2*2)\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    # forward propagation parameters\n",
    "    ## input: input matrix / image\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.depth): # In this case 2 (number of kernel layers)\n",
    "            for j in range(self.input_depth): # In this case 3 (number of color channels)\n",
    "                self.output[i] = signal.correlate2d(self.input[j], self.kernels[i,j], \"valid\")\n",
    "        return self.output\n",
    "    \n",
    "    # backward progagation parameters\n",
    "    ## output_gradient: differential of error W.R.T output matrix\n",
    "    ## learning_rate: rate of updation of the input_gradient (how big of a step it is taking to reach global minima)\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # Lets assume input kernel is a null matrix\n",
    "        kernel_gradient = np.zeros(self.kernels_shape)\n",
    "        # Lets assume that the initial value of input gradient, i.e the amount of correction made to the value of input to be a null matrix\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth): # In this case 2 (number of kernel layers)\n",
    "            for j in range(self.input_depth): # In this case 3 (number of color channels)\n",
    "                # kernel gradient is calculated as cross-correlation of input matrix and output gradient (refer notes to understand how we arrived at this formula)\n",
    "                kernel_gradient[i,j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
    "                #input gradient is calculated as convolution of output gradient and kernel (refer notes to understand how we arrived at this formula)\n",
    "                input_gradient[j] = signal.convolve2d(output_gradient[i], self.kernels[i,j],\"full\")\n",
    "        \n",
    "        # kernels and biases has to be updated as computed above according to the learning rate provided\n",
    "        self.kernels -= learning_rate * kernel_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "\n",
    "        # input_gradient returned in the process of back propagation is used to update the value of input_gradient (error correction)\n",
    "        return input_gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer notes to see why reshape is used\n",
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer notes to see why binary cross entropy is used and how we arrived at these formulas\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(y_pred))\n",
    "\n",
    "def binary_cross_entropy_prime(y_true, y_pred):\n",
    "    return ((1 - y_true) / (1 - y_pred) - (y_true / y_pred)) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        # Refer notes to see how we calculated derivative of sigmoid function\n",
    "        def sigmoid_prime(x):\n",
    "            return sigmoid(x) * (1 - sigmoid(x))\n",
    "        \n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
